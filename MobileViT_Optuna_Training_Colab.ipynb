{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# MobileViT Training with Optuna Optimization\n",
                "\n",
                "This notebook trains a MobileViT model on the Kvasir-V2 dataset with Optuna TPE hyperparameter optimization.\n",
                "\n",
                "## Features:\n",
                "- Custom MobileViT architecture implementation\n",
                "- Optuna TPE hyperparameter optimization\n",
                "- Comprehensive metrics tracking\n",
                "- ROC curve analysis\n",
                "\n",
                "## Setup Instructions:\n",
                "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí GPU (T4 or better)\n",
                "2. **Run all cells sequentially**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Check GPU Availability"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPU\n",
                "!nvidia-smi\n",
                "\n",
                "import torch\n",
                "\n",
                "print(\"\\nPyTorch version:\", torch.__version__)\n",
                "print(\"CUDA available:\", torch.cuda.is_available())\n",
                "if torch.cuda.is_available():\n",
                "    print(\"CUDA version:\", torch.version.cuda)\n",
                "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
                "    print(\"GPU count:\", torch.cuda.device_count())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Install Required Packages"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
                "!pip install -q optuna\n",
                "!pip install -q scikit-learn matplotlib seaborn pandas numpy tqdm\n",
                "\n",
                "print(\"‚úÖ All packages installed successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Download Kvasir-V2 Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import zipfile\n",
                "import urllib.request\n",
                "\n",
                "# Download Kvasir-V2 dataset\n",
                "dataset_url = \"https://datasets.simula.no/downloads/kvasir/kvasir-dataset-v2.zip\"\n",
                "dataset_zip = \"kvasir-dataset-v2.zip\"\n",
                "\n",
                "if not os.path.exists(\"kvasir-dataset-v2\"):\n",
                "    print(\"Downloading Kvasir dataset...\")\n",
                "    urllib.request.urlretrieve(dataset_url, dataset_zip)\n",
                "    \n",
                "    print(\"Extracting dataset...\")\n",
                "    with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:\n",
                "        zip_ref.extractall(\".\")\n",
                "    \n",
                "    os.remove(dataset_zip)\n",
                "    print(\"‚úÖ Dataset ready!\")\n",
                "else:\n",
                "    print(\"‚úÖ Dataset already exists!\")\n",
                "\n",
                "# Verify dataset\n",
                "data_dir = \"kvasir-dataset-v2/kvasir-dataset-v2\"\n",
                "classes = sorted(os.listdir(data_dir))\n",
                "print(f\"\\nFound {len(classes)} classes: {classes}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Define MobileViT Architecture"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "\n",
                "# Helper functions\n",
                "def conv_1x1_bn(inp, oup):\n",
                "    return nn.Sequential(\n",
                "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
                "        nn.BatchNorm2d(oup),\n",
                "        nn.Hardswish()\n",
                "    )\n",
                "\n",
                "def conv_nxn_bn(inp, oup, kernal_size=3, stride=1):\n",
                "    return nn.Sequential(\n",
                "        nn.Conv2d(inp, oup, kernal_size, stride, 1, bias=False),\n",
                "        nn.BatchNorm2d(oup),\n",
                "        nn.Hardswish()\n",
                "    )\n",
                "\n",
                "class PreNorm(nn.Module):\n",
                "    def __init__(self, dim, fn):\n",
                "        super().__init__()\n",
                "        self.norm = nn.LayerNorm(dim)\n",
                "        self.fn = fn\n",
                "    \n",
                "    def forward(self, x, *args, **kwargs):\n",
                "        return self.fn(self.norm(x), *args, **kwargs)\n",
                "\n",
                "class FeedForward(nn.Module):\n",
                "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(dim, hidden_dim),\n",
                "            nn.Hardswish(),\n",
                "            nn.Dropout(dropout),\n",
                "            nn.Linear(hidden_dim, dim),\n",
                "            nn.Dropout(dropout)\n",
                "        )\n",
                "    \n",
                "    def forward(self, x):\n",
                "        return self.net(x)\n",
                "\n",
                "class Attention(nn.Module):\n",
                "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
                "        super().__init__()\n",
                "        self.inner_dim = dim_head * heads\n",
                "        project_out = not (heads == 1 and dim_head == dim)\n",
                "\n",
                "        self.heads = heads\n",
                "        self.scale = dim_head ** -0.5\n",
                "\n",
                "        self.attend = nn.Softmax(dim=-1)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "\n",
                "        self.to_qkv = nn.Linear(dim, self.inner_dim * 3, bias=False)\n",
                "\n",
                "        self.to_out = nn.Sequential(\n",
                "            nn.Linear(self.inner_dim, dim),\n",
                "            nn.Dropout(dropout)\n",
                "        ) if project_out else nn.Identity()\n",
                "\n",
                "    def forward(self, x):\n",
                "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
                "        q, k, v = map(lambda t: t.reshape(t.shape[0], t.shape[1], self.heads, t.shape[2] // self.heads).transpose(1, 2), qkv)\n",
                "\n",
                "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
                "        attn = self.attend(dots)\n",
                "        attn = self.dropout(attn)\n",
                "\n",
                "        out = torch.matmul(attn, v)\n",
                "        out = out.transpose(1, 2).reshape(out.shape[0], out.shape[1], self.inner_dim)\n",
                "        return self.to_out(out)\n",
                "\n",
                "class TransformerBlock(nn.Module):\n",
                "    def __init__(self, dim, heads, dim_head, mlp_dim, dropout=0.):\n",
                "        super().__init__()\n",
                "        self.attn = PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout))\n",
                "        self.ff = PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = x + self.attn(x)\n",
                "        x = x + self.ff(x)\n",
                "        return x\n",
                "\n",
                "class MobileNetV2Block(nn.Module):\n",
                "    def __init__(self, inp, oup, stride, expansion):\n",
                "        super().__init__()\n",
                "        self.stride = stride\n",
                "        hidden_dim = int(round(inp * expansion))\n",
                "        self.use_res_connect = self.stride == 1 and inp == oup\n",
                "\n",
                "        if expansion == 1:\n",
                "            self.conv = nn.Sequential(\n",
                "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
                "                nn.BatchNorm2d(hidden_dim),\n",
                "                nn.Hardswish(),\n",
                "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
                "                nn.BatchNorm2d(oup),\n",
                "            )\n",
                "        else:\n",
                "            self.conv = nn.Sequential(\n",
                "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
                "                nn.BatchNorm2d(hidden_dim),\n",
                "                nn.Hardswish(),\n",
                "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
                "                nn.BatchNorm2d(hidden_dim),\n",
                "                nn.Hardswish(),\n",
                "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
                "                nn.BatchNorm2d(oup),\n",
                "            )\n",
                "\n",
                "    def forward(self, x):\n",
                "        if self.use_res_connect:\n",
                "            return x + self.conv(x)\n",
                "        else:\n",
                "            return self.conv(x)\n",
                "\n",
                "class MobileViTBlock(nn.Module):\n",
                "    def __init__(self, dim, depth, channel, kernal_size, patch_size, mlp_dim, dropout=0.):\n",
                "        super().__init__()\n",
                "        self.ph, self.pw = patch_size\n",
                "\n",
                "        self.conv1 = conv_nxn_bn(channel, channel, kernal_size)\n",
                "        self.conv2 = conv_1x1_bn(channel, dim)\n",
                "        self.transformer = nn.Sequential(*[TransformerBlock(dim, 4, 8, mlp_dim, dropout) for _ in range(depth)])\n",
                "        self.conv3 = conv_1x1_bn(dim, channel)\n",
                "        self.conv4 = conv_nxn_bn(2 * channel, channel, kernal_size)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        y = x.clone()\n",
                "        x = self.conv1(x)\n",
                "        x = self.conv2(x)\n",
                "        \n",
                "        shape = x.shape\n",
                "        B, D, H, W = shape\n",
                "\n",
                "        pad_h = (self.ph - (H % self.ph)) % self.ph\n",
                "        pad_w = (self.pw - (W % self.pw)) % self.pw\n",
                "\n",
                "        if pad_h > 0 or pad_w > 0:\n",
                "            x = F.pad(x, (0, pad_w, 0, pad_h), mode='constant', value=0)\n",
                "            H = H + pad_h\n",
                "            W = W + pad_w\n",
                "\n",
                "        h_split, w_split = H // self.ph, W // self.pw\n",
                "\n",
                "        x = x.reshape(B, D, h_split, self.ph, w_split, self.pw)\n",
                "        x = x.permute(0, 2, 4, 3, 5, 1).contiguous()\n",
                "        x = x.view(B * h_split * w_split, self.ph * self.pw, D)\n",
                "\n",
                "        x = self.transformer(x)\n",
                "\n",
                "        x = x.view(B, h_split, w_split, self.ph, self.pw, D)\n",
                "        x = x.permute(0, 5, 1, 3, 2, 4).contiguous()\n",
                "        x = x.view(B, D, H, W)\n",
                "\n",
                "        if pad_h > 0 or pad_w > 0:\n",
                "            x = x[:, :, :shape[2], :shape[3]]\n",
                "\n",
                "        x = self.conv3(x)\n",
                "        x = torch.cat((x, y), 1)\n",
                "        x = self.conv4(x)\n",
                "        return x\n",
                "\n",
                "class MobileViT(nn.Module):\n",
                "    def __init__(self, image_size, num_classes, dims, channels, depths, patch_size=2, mlp_dim_ratio=2):\n",
                "        super().__init__()\n",
                "        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
                "        \n",
                "        ph, pw = patch_size, patch_size\n",
                "\n",
                "        self.conv1 = conv_nxn_bn(3, channels[0], kernal_size=3, stride=2)\n",
                "\n",
                "        self.stem = nn.ModuleList([])\n",
                "        in_channel = channels[0]\n",
                "\n",
                "        self.stem.append(MobileNetV2Block(in_channel, channels[1], stride=1, expansion=4))\n",
                "        in_channel = channels[1]\n",
                "        self.stem.append(MobileNetV2Block(in_channel, channels[2], stride=2, expansion=4))\n",
                "        in_channel = channels[2]\n",
                "        self.stem.append(MobileNetV2Block(in_channel, channels[3], stride=1, expansion=4))\n",
                "        in_channel = channels[3]\n",
                "        self.stem.append(MobileNetV2Block(in_channel, channels[4], stride=2, expansion=4))\n",
                "        in_channel = channels[4]\n",
                "\n",
                "        self.mobilevit_blocks = nn.ModuleList([])\n",
                "        for i in range(len(dims)):\n",
                "            mlp_dim = dims[i] * mlp_dim_ratio\n",
                "            self.mobilevit_blocks.append(MobileViTBlock(dims[i], depths[i], in_channel, 3, (ph, pw), mlp_dim))\n",
                "            \n",
                "            if i < len(dims) - 1:\n",
                "                next_channel_idx = i + 5\n",
                "                self.mobilevit_blocks.append(MobileNetV2Block(in_channel, channels[next_channel_idx], stride=2, expansion=4))\n",
                "                in_channel = channels[next_channel_idx]\n",
                "\n",
                "        self.conv2 = conv_1x1_bn(in_channel, dims[-1])\n",
                "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
                "        self.fc = nn.Linear(dims[-1], num_classes)\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = self.conv1(x)\n",
                "        for block in self.stem:\n",
                "            x = block(x)\n",
                "        \n",
                "        for block in self.mobilevit_blocks:\n",
                "            x = block(x)\n",
                "\n",
                "        x = self.conv2(x)\n",
                "        x = self.pool(x).flatten(1)\n",
                "        x = self.fc(x)\n",
                "        return x\n",
                "\n",
                "print(\"‚úÖ MobileViT architecture defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Define Dataset Class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from PIL import Image\n",
                "from torch.utils.data import Dataset\n",
                "\n",
                "class KvasirDataset(Dataset):\n",
                "    def __init__(self, root_dir, transform=None):\n",
                "        self.root_dir = root_dir\n",
                "        self.transform = transform\n",
                "        self.classes = sorted(os.listdir(root_dir))\n",
                "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
                "        self.image_paths = []\n",
                "        self.labels = []\n",
                "\n",
                "        for class_name in self.classes:\n",
                "            class_dir = os.path.join(root_dir, class_name)\n",
                "            if not os.path.isdir(class_dir):\n",
                "                continue\n",
                "            for img_name in os.listdir(class_dir):\n",
                "                if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
                "                    self.image_paths.append(os.path.join(class_dir, img_name))\n",
                "                    self.labels.append(self.class_to_idx[class_name])\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.image_paths)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        img_path = self.image_paths[idx]\n",
                "        image = Image.open(img_path).convert('RGB')\n",
                "        label = self.labels[idx]\n",
                "\n",
                "        if self.transform:\n",
                "            image = self.transform(image)\n",
                "\n",
                "        return image, label\n",
                "\n",
                "print(\"‚úÖ Dataset class defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Define Training and Validation Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tqdm import tqdm\n",
                "from sklearn.metrics import accuracy_score\n",
                "import torch.optim as optim\n",
                "\n",
                "def train(model, dataloader, criterion, optimizer, device):\n",
                "    model.train()\n",
                "    running_loss = 0.0\n",
                "    all_labels = []\n",
                "    all_predictions = []\n",
                "\n",
                "    for images, labels in tqdm(dataloader, desc=\"Training\"):\n",
                "        images, labels = images.to(device), labels.to(device)\n",
                "\n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(images)\n",
                "        loss = criterion(outputs, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "\n",
                "        running_loss += loss.item()\n",
                "        _, predicted = torch.max(outputs.data, 1)\n",
                "        all_labels.extend(labels.cpu().numpy())\n",
                "        all_predictions.extend(predicted.cpu().numpy())\n",
                "    \n",
                "    avg_loss = running_loss / len(dataloader)\n",
                "    accuracy = accuracy_score(all_labels, all_predictions)\n",
                "    return avg_loss, accuracy\n",
                "\n",
                "def validate(model, dataloader, criterion, device):\n",
                "    model.eval()\n",
                "    running_loss = 0.0\n",
                "    all_labels = []\n",
                "    all_predictions = []\n",
                "    all_probabilities = []\n",
                "\n",
                "    with torch.no_grad():\n",
                "        for images, labels in tqdm(dataloader, desc=\"Validation\"):\n",
                "            images, labels = images.to(device), labels.to(device)\n",
                "            outputs = model(images)\n",
                "            loss = criterion(outputs, labels)\n",
                "            running_loss += loss.item()\n",
                "            \n",
                "            probabilities = F.softmax(outputs, dim=1)\n",
                "            _, predicted = torch.max(outputs.data, 1)\n",
                "\n",
                "            all_labels.extend(labels.cpu().numpy())\n",
                "            all_predictions.extend(predicted.cpu().numpy())\n",
                "            all_probabilities.extend(probabilities.cpu().numpy())\n",
                "    \n",
                "    avg_loss = running_loss / len(dataloader)\n",
                "    accuracy = accuracy_score(all_labels, all_predictions)\n",
                "    return avg_loss, accuracy, all_labels, all_probabilities\n",
                "\n",
                "print(\"‚úÖ Training functions defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Define Optuna Objective Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import optuna\n",
                "import numpy as np\n",
                "from torch.utils.data import DataLoader, random_split\n",
                "from torchvision import transforms\n",
                "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
                "import time\n",
                "\n",
                "def objective(trial):\n",
                "    # Hyperparameters to tune\n",
                "    image_size = 224\n",
                "    num_classes = 8\n",
                "    dataset_root = 'kvasir-dataset-v2/kvasir-dataset-v2'\n",
                "    \n",
                "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
                "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
                "    num_epochs = 5  # Fixed for faster optimization\n",
                "\n",
                "    # MobileViT architectural hyperparameters\n",
                "    depths = [\n",
                "        trial.suggest_int('depth_0', 1, 3),\n",
                "        trial.suggest_int('depth_1', 2, 5),\n",
                "        trial.suggest_int('depth_2', 2, 4)\n",
                "    ]\n",
                "    dims = [96, 192, 384]\n",
                "    channels = [16, 32, 64, 128, 160, 192, 256, 320, 384, 512]\n",
                "\n",
                "    # Device configuration\n",
                "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "    print(f\"Using device: {device} for trial {trial.number}\")\n",
                "    \n",
                "    # Data transformations\n",
                "    transform = transforms.Compose([\n",
                "        transforms.Resize((image_size, image_size)),\n",
                "        transforms.ToTensor(),\n",
                "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
                "    ])\n",
                "\n",
                "    # Dataset and DataLoaders\n",
                "    full_dataset = KvasirDataset(root_dir=dataset_root, transform=transform)\n",
                "    \n",
                "    # Split dataset: 70% train, 15% val, 15% test\n",
                "    train_size = int(0.7 * len(full_dataset))\n",
                "    val_size = int(0.15 * len(full_dataset))\n",
                "    test_size = len(full_dataset) - train_size - val_size\n",
                "    train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
                "\n",
                "    pin_memory = True if torch.cuda.is_available() else False\n",
                "    num_workers = 2 if torch.cuda.is_available() else 0\n",
                "    \n",
                "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
                "                             num_workers=num_workers, pin_memory=pin_memory)\n",
                "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, \n",
                "                           num_workers=num_workers, pin_memory=pin_memory)\n",
                "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, \n",
                "                            num_workers=num_workers, pin_memory=pin_memory)\n",
                "\n",
                "    # Model initialization\n",
                "    model = MobileViT(image_size=image_size, num_classes=num_classes, \n",
                "                     dims=dims, channels=channels, depths=depths).to(device)\n",
                "\n",
                "    # Loss and optimizer\n",
                "    criterion = nn.CrossEntropyLoss()\n",
                "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
                "\n",
                "    best_val_accuracy = 0.0\n",
                "    model_save_path = f\"mobilevit_kvasir_v2_trial_{trial.number}.pth\"\n",
                "\n",
                "    # Training loop\n",
                "    for epoch in range(num_epochs):\n",
                "        train_loss, train_accuracy = train(model, train_loader, criterion, optimizer, device)\n",
                "        val_loss, val_accuracy, _, _ = validate(model, val_loader, criterion, device)\n",
                "\n",
                "        print(f\"  Trial {trial.number}, Epoch {epoch+1}/{num_epochs}: \"\n",
                "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f} | \"\n",
                "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
                "\n",
                "        if val_accuracy > best_val_accuracy:\n",
                "            best_val_accuracy = val_accuracy\n",
                "            torch.save(model.state_dict(), model_save_path)\n",
                "\n",
                "        trial.report(val_accuracy, epoch)\n",
                "\n",
                "        if trial.should_prune():\n",
                "            raise optuna.exceptions.TrialPruned()\n",
                "\n",
                "    # Load best model for final evaluation\n",
                "    model.load_state_dict(torch.load(model_save_path))\n",
                "    \n",
                "    # Test evaluation\n",
                "    test_loss, test_accuracy, test_labels, test_probs = validate(model, test_loader, criterion, device)\n",
                "    \n",
                "    test_preds = np.argmax(test_probs, axis=1)\n",
                "    test_precision = precision_score(test_labels, test_preds, average='weighted')\n",
                "    test_recall = recall_score(test_labels, test_preds, average='weighted')\n",
                "    test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
                "\n",
                "    print(f\"\\n--- Trial {trial.number} Results ---\")\n",
                "    print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
                "    print(f\"  Test Precision: {test_precision:.4f}\")\n",
                "    print(f\"  Test Recall: {test_recall:.4f}\")\n",
                "    print(f\"  Test F1-score: {test_f1:.4f}\")\n",
                "    print(\"------------------------------------\\n\")\n",
                "\n",
                "    return best_val_accuracy\n",
                "\n",
                "print(\"‚úÖ Optuna objective function defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Run Optuna Optimization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nüöÄ Starting Optuna optimization...\\n\")\n",
                "\n",
                "study = optuna.create_study(\n",
                "    direction=\"maximize\",\n",
                "    pruner=optuna.pruners.MedianPruner(),\n",
                "    study_name=\"mobilevit_optimization\"\n",
                ")\n",
                "\n",
                "study.optimize(objective, n_trials=15)\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"OPTUNA OPTIMIZATION COMPLETE\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Number of finished trials: {len(study.trials)}\")\n",
                "print(f\"\\nBest trial:\")\n",
                "print(f\"  Value (Val Accuracy): {study.best_value:.4f}\")\n",
                "print(f\"\\n  Best Hyperparameters:\")\n",
                "for key, value in study.best_params.items():\n",
                "    print(f\"    {key}: {value}\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Save Optimization Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Save all trials to CSV\n",
                "trials_df = study.trials_dataframe()\n",
                "trials_df.to_csv('optuna_study_results.csv', index=False)\n",
                "\n",
                "print(\"‚úÖ Optimization results saved to 'optuna_study_results.csv'\")\n",
                "\n",
                "# Display top 5 trials\n",
                "print(\"\\nTop 5 Trials:\")\n",
                "print(trials_df.nlargest(5, 'value')[['number', 'value', 'params_batch_size', \n",
                "                                       'params_learning_rate', 'params_depth_0', \n",
                "                                       'params_depth_1', 'params_depth_2']])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Plot Optimization History"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
                "\n",
                "# Plot optimization history\n",
                "axes[0].plot([t.number for t in study.trials], [t.value for t in study.trials], 'o-')\n",
                "axes[0].axhline(y=study.best_value, color='r', linestyle='--', label=f'Best: {study.best_value:.4f}')\n",
                "axes[0].set_xlabel('Trial Number')\n",
                "axes[0].set_ylabel('Validation Accuracy')\n",
                "axes[0].set_title('Optuna Optimization History')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Plot parameter importance (if available)\n",
                "try:\n",
                "    importance = optuna.importance.get_param_importances(study)\n",
                "    params = list(importance.keys())\n",
                "    values = list(importance.values())\n",
                "    \n",
                "    axes[1].barh(params, values)\n",
                "    axes[1].set_xlabel('Importance')\n",
                "    axes[1].set_title('Hyperparameter Importance')\n",
                "    axes[1].grid(True, alpha=0.3, axis='x')\n",
                "except:\n",
                "    axes[1].text(0.5, 0.5, 'Parameter importance\\nnot available', \n",
                "                ha='center', va='center', fontsize=12)\n",
                "    axes[1].axis('off')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('optuna_history.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n‚úÖ Optimization plots saved!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Save Best Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the best model\n",
                "best_trial = study.best_trial\n",
                "best_model_path = \"mobilevit_kvasir_v2_best_optuna.pth\"\n",
                "\n",
                "# Copy the best trial's model\n",
                "import shutil\n",
                "source_path = f\"mobilevit_kvasir_v2_trial_{best_trial.number}.pth\"\n",
                "if os.path.exists(source_path):\n",
                "    shutil.copy(source_path, best_model_path)\n",
                "    print(f\"‚úÖ Best model saved to {best_model_path}\")\n",
                "    print(f\"   (from trial {best_trial.number})\")\n",
                "else:\n",
                "    print(f\"‚ö†Ô∏è Best model file not found: {source_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Download Results (Optional)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Uncomment to download files to your local machine\n",
                "# from google.colab import files\n",
                "\n",
                "# files.download('mobilevit_kvasir_v2_best_optuna.pth')\n",
                "# files.download('optuna_study_results.csv')\n",
                "# files.download('optuna_history.png')\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"üéâ MOBILEVIT TRAINING COMPLETE!\")\n",
                "print(\"=\"*60)\n",
                "print(\"\\nGenerated files:\")\n",
                "print(\"  - mobilevit_kvasir_v2_best_optuna.pth (Best model)\")\n",
                "print(\"  - optuna_study_results.csv (All trials)\")\n",
                "print(\"  - optuna_history.png (Optimization plots)\")\n",
                "print(\"  - mobilevit_kvasir_v2_trial_*.pth (Individual trial models)\")\n",
                "print(\"=\"*60)"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸš€ Swin-Small + MobileViT (Kvasir) with Optuna TPE\n",
                "\n",
                "## Architecture\n",
                "- **Swin-Small**: 768-dim features (ImageNet pretrained)\n",
                "- **MobileViT**: 512-dim features (**Kvasir pretrained** - custom architecture)\n",
                "- **Fusion**: 768 + 512 = 1280-dim -> 256-dim\n",
                "- **Classifier**: CatBoost with Optuna TPE optimization\n",
                "\n",
                "## Expected Results\n",
                "- **Accuracy**: ~92-93% (with Kvasir MobileViT)\n",
                "- **AUROC**: ~99%\n",
                "- **Training Time**: ~40-45 minutes\n",
                "\n",
                "**Note**: This notebook includes the custom MobileViT architecture to load Kvasir-pretrained weights."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install packages\n",
                "!pip install -q optuna catboost scikit-learn\n",
                "print(\"âœ… Packages installed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mount Google Drive for MobileViT weights\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "import os\n",
                "weights_path = '/content/drive/MyDrive/TripleHybridModel/mobilevit_kvasir_v2_best_optuna.pth'\n",
                "\n",
                "if os.path.exists(weights_path):\n",
                "    print(f\"âœ… Found MobileViT Kvasir weights\")\n",
                "else:\n",
                "    print(f\"âŒ Weights not found. Upload to: MyDrive/TripleHybridModel/\")\n",
                "    weights_path = None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torchvision.models as models\n",
                "from torchvision import transforms\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from PIL import Image\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "from tqdm.auto import tqdm\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, log_loss\n",
                "from sklearn.preprocessing import label_binarize\n",
                "import optuna\n",
                "from catboost import CatBoostClassifier\n",
                "import time\n",
                "import json\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"ðŸŽ® Device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download dataset\n",
                "if not os.path.exists('kvasir-dataset-v2'):\n",
                "    print(\"ðŸ“¥ Downloading Kvasir dataset...\")\n",
                "    !wget --no-check-certificate https://datasets.simula.no/downloads/kvasir/kvasir-dataset-v2.zip\n",
                "    print(\"ðŸ“¦ Extracting...\")\n",
                "    !unzip -q kvasir-dataset-v2.zip\n",
                "    print(\"âœ… Dataset ready!\")\n",
                "else:\n",
                "    print(\"âœ… Dataset already exists\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ—ï¸ Custom MobileViT Architecture (for Kvasir weights)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Helper functions\n",
                "def conv_1x1_bn(inp, oup):\n",
                "    return nn.Sequential(\n",
                "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
                "        nn.BatchNorm2d(oup),\n",
                "        nn.Hardswish()\n",
                "    )\n",
                "\n",
                "def conv_nxn_bn(inp, oup, kernal_size=3, stride=1):\n",
                "    return nn.Sequential(\n",
                "        nn.Conv2d(inp, oup, kernal_size, stride, 1, bias=False),\n",
                "        nn.BatchNorm2d(oup),\n",
                "        nn.Hardswish()\n",
                "    )\n",
                "\n",
                "class PreNorm(nn.Module):\n",
                "    def __init__(self, dim, fn):\n",
                "        super().__init__()\n",
                "        self.norm = nn.LayerNorm(dim)\n",
                "        self.fn = fn\n",
                "    def forward(self, x, *args, **kwargs):\n",
                "        return self.fn(self.norm(x), *args, **kwargs)\n",
                "\n",
                "class FeedForward(nn.Module):\n",
                "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(dim, hidden_dim),\n",
                "            nn.Hardswish(),\n",
                "            nn.Dropout(dropout),\n",
                "            nn.Linear(hidden_dim, dim),\n",
                "            nn.Dropout(dropout)\n",
                "        )\n",
                "    def forward(self, x):\n",
                "        return self.net(x)\n",
                "\n",
                "class Attention(nn.Module):\n",
                "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
                "        super().__init__()\n",
                "        self.inner_dim = dim_head * heads\n",
                "        project_out = not (heads == 1 and dim_head == dim)\n",
                "        self.heads = heads\n",
                "        self.scale = dim_head ** -0.5\n",
                "        self.attend = nn.Softmax(dim=-1)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "        self.to_qkv = nn.Linear(dim, self.inner_dim * 3, bias=False)\n",
                "        self.to_out = nn.Sequential(\n",
                "            nn.Linear(self.inner_dim, dim),\n",
                "            nn.Dropout(dropout)\n",
                "        ) if project_out else nn.Identity()\n",
                "    \n",
                "    def forward(self, x):\n",
                "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
                "        q, k, v = map(lambda t: t.reshape(t.shape[0], t.shape[1], self.heads, t.shape[2] // self.heads).transpose(1, 2), qkv)\n",
                "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
                "        attn = self.attend(dots)\n",
                "        attn = self.dropout(attn)\n",
                "        out = torch.matmul(attn, v)\n",
                "        out = out.transpose(1, 2).reshape(out.shape[0], out.shape[1], self.inner_dim)\n",
                "        return self.to_out(out)\n",
                "\n",
                "class TransformerBlock(nn.Module):\n",
                "    def __init__(self, dim, heads, dim_head, mlp_dim, dropout=0.):\n",
                "        super().__init__()\n",
                "        self.attn = PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout))\n",
                "        self.ff = PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = x + self.attn(x)\n",
                "        x = x + self.ff(x)\n",
                "        return x\n",
                "\n",
                "class MobileNetV2Block(nn.Module):\n",
                "    def __init__(self, inp, oup, stride, expansion):\n",
                "        super().__init__()\n",
                "        self.stride = stride\n",
                "        hidden_dim = int(round(inp * expansion))\n",
                "        self.use_res_connect = self.stride == 1 and inp == oup\n",
                "        \n",
                "        if expansion == 1:\n",
                "            self.conv = nn.Sequential(\n",
                "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
                "                nn.BatchNorm2d(hidden_dim),\n",
                "                nn.Hardswish(),\n",
                "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
                "                nn.BatchNorm2d(oup),\n",
                "            )\n",
                "        else:\n",
                "            self.conv = nn.Sequential(\n",
                "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
                "                nn.BatchNorm2d(hidden_dim),\n",
                "                nn.Hardswish(),\n",
                "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
                "                nn.BatchNorm2d(hidden_dim),\n",
                "                nn.Hardswish(),\n",
                "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
                "                nn.BatchNorm2d(oup),\n",
                "            )\n",
                "    \n",
                "    def forward(self, x):\n",
                "        if self.use_res_connect:\n",
                "            return x + self.conv(x)\n",
                "        else:\n",
                "            return self.conv(x)\n",
                "\n",
                "class MobileViTBlock(nn.Module):\n",
                "    def __init__(self, dim, depth, channel, kernal_size, patch_size, mlp_dim, dropout=0.):\n",
                "        super().__init__()\n",
                "        self.ph, self.pw = patch_size\n",
                "        self.conv1 = conv_nxn_bn(channel, channel, kernal_size)\n",
                "        self.conv2 = conv_1x1_bn(channel, dim)\n",
                "        self.transformer = nn.Sequential(*[TransformerBlock(dim, 4, 8, mlp_dim, dropout) for _ in range(depth)])\n",
                "        self.conv3 = conv_1x1_bn(dim, channel)\n",
                "        self.conv4 = conv_nxn_bn(2 * channel, channel, kernal_size)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        y = x.clone()\n",
                "        x = self.conv1(x)\n",
                "        x = self.conv2(x)\n",
                "        \n",
                "        B, D, H, W = x.shape\n",
                "        pad_h = (self.ph - (H % self.ph)) % self.ph\n",
                "        pad_w = (self.pw - (W % self.pw)) % self.pw\n",
                "        \n",
                "        if pad_h > 0 or pad_w > 0:\n",
                "            x = F.pad(x, (0, pad_w, 0, pad_h), mode='constant', value=0)\n",
                "            H, W = H + pad_h, W + pad_w\n",
                "        \n",
                "        h_split, w_split = H // self.ph, W // self.pw\n",
                "        x = x.reshape(B, D, h_split, self.ph, w_split, self.pw)\n",
                "        x = x.permute(0, 2, 4, 3, 5, 1).contiguous()\n",
                "        x = x.view(B * h_split * w_split, self.ph * self.pw, D)\n",
                "        x = self.transformer(x)\n",
                "        x = x.view(B, h_split, w_split, self.ph, self.pw, D)\n",
                "        x = x.permute(0, 5, 1, 3, 2, 4).contiguous()\n",
                "        x = x.view(B, D, H, W)\n",
                "        \n",
                "        if pad_h > 0 or pad_w > 0:\n",
                "            x = x[:, :, :H-pad_h, :W-pad_w]\n",
                "        \n",
                "        x = self.conv3(x)\n",
                "        x = torch.cat((x, y), 1)\n",
                "        x = self.conv4(x)\n",
                "        return x\n",
                "\n",
                "class MobileViT(nn.Module):\n",
                "    def __init__(self, image_size, num_classes, dims, channels, depths, patch_size=2, mlp_dim_ratio=2):\n",
                "        super().__init__()\n",
                "        ph, pw = patch_size, patch_size\n",
                "        self.conv1 = conv_nxn_bn(3, channels[0], kernal_size=3, stride=2)\n",
                "        \n",
                "        self.stem = nn.ModuleList([])\n",
                "        in_channel = channels[0]\n",
                "        self.stem.append(MobileNetV2Block(in_channel, channels[1], stride=1, expansion=4))\n",
                "        in_channel = channels[1]\n",
                "        self.stem.append(MobileNetV2Block(in_channel, channels[2], stride=2, expansion=4))\n",
                "        in_channel = channels[2]\n",
                "        self.stem.append(MobileNetV2Block(in_channel, channels[3], stride=1, expansion=4))\n",
                "        in_channel = channels[3]\n",
                "        self.stem.append(MobileNetV2Block(in_channel, channels[4], stride=2, expansion=4))\n",
                "        in_channel = channels[4]\n",
                "        \n",
                "        self.mobilevit_blocks = nn.ModuleList([])\n",
                "        for i in range(len(dims)):\n",
                "            mlp_dim = dims[i] * mlp_dim_ratio\n",
                "            self.mobilevit_blocks.append(MobileViTBlock(dims[i], depths[i], in_channel, 3, (ph, pw), mlp_dim))\n",
                "            if i < len(dims) - 1:\n",
                "                next_channel_idx = i + 5\n",
                "                self.mobilevit_blocks.append(MobileNetV2Block(in_channel, channels[next_channel_idx], stride=2, expansion=4))\n",
                "                in_channel = channels[next_channel_idx]\n",
                "        \n",
                "        self.conv2 = conv_1x1_bn(in_channel, dims[-1])\n",
                "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
                "        self.fc = nn.Linear(dims[-1], num_classes)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = self.conv1(x)\n",
                "        for block in self.stem:\n",
                "            x = block(x)\n",
                "        for block in self.mobilevit_blocks:\n",
                "            x = block(x)\n",
                "        x = self.conv2(x)\n",
                "        x = self.pool(x).flatten(1)\n",
                "        x = self.fc(x)\n",
                "        return x\n",
                "\n",
                "print(\"âœ… Custom MobileViT architecture defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ—ï¸ Hybrid Model with Kvasir MobileViT"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SwinSmallMobileViTHybrid(nn.Module):\n",
                "    def __init__(self, mobilevit_weights_path=None):\n",
                "        super().__init__()\n",
                "        \n",
                "        # Swin-Small\n",
                "        print(\"Loading Swin-Small...\")\n",
                "        self.swin = models.swin_s(weights=models.Swin_S_Weights.IMAGENET1K_V1)\n",
                "        self.swin.head = nn.Identity()\n",
                "        self.swin_dim = 768\n",
                "        \n",
                "        # Custom MobileViT\n",
                "        print(\"Loading Custom MobileViT...\")\n",
                "        dims = [96, 192, 384]\n",
                "        channels = [16, 32, 64, 128, 160, 192, 256, 320, 384, 512]\n",
                "        depths = [2, 2, 2]\n",
                "        \n",
                "        self.mobilevit = MobileViT(\n",
                "            image_size=224,\n",
                "            num_classes=8,\n",
                "            dims=dims,\n",
                "            channels=channels,\n",
                "            depths=depths\n",
                "        )\n",
                "        \n",
                "        # Load Kvasir weights\n",
                "        if mobilevit_weights_path and os.path.exists(mobilevit_weights_path):\n",
                "            self.mobilevit.load_state_dict(torch.load(mobilevit_weights_path, map_location='cpu'))\n",
                "            print(\"âœ… MobileViT Kvasir weights loaded!\")\n",
                "        else:\n",
                "            print(\"âš ï¸ No Kvasir weights - using random initialization\")\n",
                "        \n",
                "        # Remove classification heads\n",
                "        self.mobilevit.fc = nn.Identity()\n",
                "        self.mobilevit_dim = 384  # Last dim from dims\n",
                "        \n",
                "        # Freeze backbones\n",
                "        for param in self.swin.parameters():\n",
                "            param.requires_grad = False\n",
                "        for param in self.mobilevit.parameters():\n",
                "            param.requires_grad = False\n",
                "        \n",
                "        # Fusion layer\n",
                "        combined_dim = self.swin_dim + self.mobilevit_dim  # 768 + 384 = 1152\n",
                "        self.fusion = nn.Sequential(\n",
                "            nn.Linear(combined_dim, 512),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(0.3),\n",
                "            nn.Linear(512, 256)\n",
                "        )\n",
                "        \n",
                "        print(f\"âœ… Hybrid Model Ready: {self.swin_dim} + {self.mobilevit_dim} = {combined_dim} -> 256\")\n",
                "    \n",
                "    def forward(self, x):\n",
                "        with torch.no_grad():\n",
                "            swin_features = self.swin(x)\n",
                "            mobilevit_features = self.mobilevit(x)\n",
                "            combined = torch.cat([swin_features, mobilevit_features], dim=1)\n",
                "        return self.fusion(combined)\n",
                "\n",
                "print(\"âœ… Hybrid model class defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dataset\n",
                "class KvasirDataset(Dataset):\n",
                "    def __init__(self, root_dir, transform=None):\n",
                "        self.root_dir = Path(root_dir)\n",
                "        self.transform = transform\n",
                "        self.samples = []\n",
                "        classes = sorted([d.name for d in self.root_dir.iterdir() if d.is_dir()])\n",
                "        self.class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
                "        for cls in classes:\n",
                "            for img in (self.root_dir / cls).glob('*.jpg'):\n",
                "                self.samples.append((img, self.class_to_idx[cls]))\n",
                "    \n",
                "    def __len__(self): return len(self.samples)\n",
                "    def __getitem__(self, idx):\n",
                "        img, label = self.samples[idx]\n",
                "        return self.transform(Image.open(img).convert('RGB')), label\n",
                "\n",
                "transform = transforms.Compose([\n",
                "    transforms.Resize((224, 224)),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
                "])\n",
                "\n",
                "if os.path.exists('kvasir-dataset-v2/kvasir-dataset-v2'):\n",
                "    dataset_path = 'kvasir-dataset-v2/kvasir-dataset-v2'\n",
                "else:\n",
                "    dataset_path = 'kvasir-dataset-v2'\n",
                "\n",
                "dataset = KvasirDataset(dataset_path, transform=transform)\n",
                "idx = list(range(len(dataset)))\n",
                "labels = [s[1] for s in dataset.samples]\n",
                "tr_idx, temp = train_test_split(idx, test_size=0.3, stratify=labels, random_state=42)\n",
                "val_idx, te_idx = train_test_split(temp, test_size=0.5, stratify=[labels[i] for i in temp], random_state=42)\n",
                "\n",
                "print(f\"ðŸ“Š Dataset: Train={len(tr_idx)}, Val={len(val_idx)}, Test={len(te_idx)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract features\n",
                "model = SwinSmallMobileViTHybrid(weights_path).to(device).eval()\n",
                "\n",
                "def extract_features(indices, set_name):\n",
                "    loader = DataLoader(torch.utils.data.Subset(dataset, indices), batch_size=32, num_workers=2)\n",
                "    feats, lbls = [], []\n",
                "    start_time = time.time()\n",
                "    for img, lbl in tqdm(loader, desc=f\"Extracting {set_name}\"):\n",
                "        feats.append(model(img.to(device)).cpu().detach().numpy())\n",
                "        lbls.append(lbl.numpy())\n",
                "    return np.vstack(feats), np.concatenate(lbls), time.time() - start_time\n",
                "\n",
                "print(\"ðŸ”¬ Extracting features...\")\n",
                "X_train, y_train, train_time = extract_features(tr_idx, \"Train\")\n",
                "X_val, y_val, val_time = extract_features(val_idx, \"Val\")\n",
                "X_test, y_test, test_time = extract_features(te_idx, \"Test\")\n",
                "\n",
                "print(f\"\\nâœ… Features: {X_train.shape}\")\n",
                "print(f\"â±ï¸ Time: Train={train_time:.1f}s, Val={val_time:.1f}s, Test={test_time:.1f}s\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Optuna TPE optimization\n",
                "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
                "\n",
                "def objective(trial):\n",
                "    params = {\n",
                "        'depth': trial.suggest_int('depth', 4, 10),\n",
                "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
                "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
                "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
                "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
                "        'task_type': 'GPU',\n",
                "        'verbose': False,\n",
                "        'random_seed': 42\n",
                "    }\n",
                "    clf = CatBoostClassifier(**params)\n",
                "    clf.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50, verbose=False)\n",
                "    return accuracy_score(y_val, clf.predict(X_val))\n",
                "\n",
                "print(\"ðŸ” Optuna TPE Optimization (50 trials)...\")\n",
                "opt_start = time.time()\n",
                "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
                "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
                "opt_time = time.time() - opt_start\n",
                "\n",
                "print(f\"\\nðŸ† Best Val Acc: {study.best_value*100:.2f}%\")\n",
                "print(f\"â±ï¸ Time: {opt_time/60:.2f} min\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train final model\n",
                "final_params = study.best_params\n",
                "final_params.update({'task_type': 'GPU', 'random_seed': 42, 'verbose': True})\n",
                "final_model = CatBoostClassifier(**final_params)\n",
                "\n",
                "train_start = time.time()\n",
                "final_model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50)\n",
                "train_time_final = time.time() - train_start\n",
                "\n",
                "print(f\"\\nâœ… Trained in {train_time_final:.2f}s\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate metrics\n",
                "def calc_metrics(model, X, y):\n",
                "    start = time.time()\n",
                "    y_pred = model.predict(X)\n",
                "    y_prob = model.predict_proba(X)\n",
                "    inf_time = time.time() - start\n",
                "    \n",
                "    y_bin = label_binarize(y, classes=range(8))\n",
                "    cm = confusion_matrix(y, y_pred)\n",
                "    tpr_list, fpr_list = [], []\n",
                "    for i in range(8):\n",
                "        tp = cm[i, i]\n",
                "        fn, fp = cm[i, :].sum() - tp, cm[:, i].sum() - tp\n",
                "        tn = cm.sum() - tp - fn - fp\n",
                "        tpr_list.append(tp / (tp + fn) if (tp + fn) > 0 else 0)\n",
                "        fpr_list.append(fp / (fp + tn) if (fp + tn) > 0 else 0)\n",
                "    \n",
                "    return {\n",
                "        'accuracy': accuracy_score(y, y_pred),\n",
                "        'precision': precision_score(y, y_pred, average='weighted', zero_division=0),\n",
                "        'recall': recall_score(y, y_pred, average='weighted', zero_division=0),\n",
                "        'f1_score': f1_score(y, y_pred, average='weighted', zero_division=0),\n",
                "        'auroc': roc_auc_score(y_bin, y_prob, average='weighted', multi_class='ovr'),\n",
                "        'tpr': np.mean(tpr_list),\n",
                "        'fpr': np.mean(fpr_list),\n",
                "        'loss': log_loss(y, y_prob),\n",
                "        'time': inf_time\n",
                "    }\n",
                "\n",
                "train_m = calc_metrics(final_model, X_train, y_train)\n",
                "val_m = calc_metrics(final_model, X_val, y_val)\n",
                "test_m = calc_metrics(final_model, X_test, y_test)\n",
                "\n",
                "print(f\"\\nTest Accuracy: {test_m['accuracy']*100:.2f}%\")\n",
                "print(f\"Test AUROC: {test_m['auroc']*100:.2f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save results\n",
                "final_model.save_model('swin_small_kvasir_optuna.cbm')\n",
                "np.savez('swin_small_kvasir_features.npz', X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test)\n",
                "\n",
                "results = {\n",
                "    'architecture': 'Swin-Small + MobileViT (Kvasir)',\n",
                "    'optimization': 'Optuna TPE',\n",
                "    'n_trials': 50,\n",
                "    'best_params': study.best_params,\n",
                "    'best_val_accuracy': study.best_value,\n",
                "    'training_metrics': train_m,\n",
                "    'validation_metrics': val_m,\n",
                "    'test_metrics': test_m,\n",
                "    'timing': {'feature_extraction': {'train': train_time, 'val': val_time, 'test': test_time}, 'optimization': opt_time, 'final_training': train_time_final}\n",
                "}\n",
                "\n",
                "with open('swin_small_kvasir_optuna_results.json', 'w') as f:\n",
                "    json.dump(results, f, indent=2, default=float)\n",
                "\n",
                "from google.colab import files\n",
                "files.download('swin_small_kvasir_optuna.cbm')\n",
                "files.download('swin_small_kvasir_features.npz')\n",
                "files.download('swin_small_kvasir_optuna_results.json')\n",
                "\n",
                "print(\"âœ… Done! Expected accuracy: ~92%\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
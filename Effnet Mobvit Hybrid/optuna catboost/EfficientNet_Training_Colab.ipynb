{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# EfficientNetV2M Training for GI Tract Classification\n",
                "\n",
                "This notebook trains an EfficientNetV2M model on the Kvasir-V2 dataset for gastrointestinal tract image classification.\n",
                "\n",
                "## Setup Instructions:\n",
                "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí GPU (T4 or better)\n",
                "2. **Run all cells sequentially**\n",
                "3. **Dataset**: Automatically downloads Kvasir-V2 dataset"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Check GPU Availability"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPU\n",
                "!nvidia-smi\n",
                "\n",
                "import tensorflow as tf\n",
                "\n",
                "print(\"\\nTensorFlow version:\", tf.__version__)\n",
                "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
                "print(\"Num GPUs:\", len(tf.config.list_physical_devices('GPU')))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Install Required Packages"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q tensorflow==2.15.0\n",
                "!pip install -q scikit-learn matplotlib seaborn pandas numpy\n",
                "\n",
                "print(\"‚úÖ All packages installed successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Download Kvasir-V2 Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import zipfile\n",
                "import urllib.request\n",
                "\n",
                "# Download Kvasir-V2 dataset\n",
                "dataset_url = \"https://datasets.simula.no/downloads/kvasir/kvasir-dataset-v2.zip\"\n",
                "dataset_zip = \"kvasir-dataset-v2.zip\"\n",
                "\n",
                "if not os.path.exists(\"kvasir-dataset-v2\"):\n",
                "    print(\"Downloading Kvasir dataset...\")\n",
                "    urllib.request.urlretrieve(dataset_url, dataset_zip)\n",
                "    \n",
                "    print(\"Extracting dataset...\")\n",
                "    with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:\n",
                "        zip_ref.extractall(\".\")\n",
                "    \n",
                "    os.remove(dataset_zip)\n",
                "    print(\"‚úÖ Dataset ready!\")\n",
                "else:\n",
                "    print(\"‚úÖ Dataset already exists!\")\n",
                "\n",
                "# Verify dataset structure\n",
                "data_dir = \"kvasir-dataset-v2/kvasir-dataset-v2/\"\n",
                "classes = sorted(os.listdir(data_dir))\n",
                "print(f\"\\nFound {len(classes)} classes: {classes}\")\n",
                "\n",
                "# Count images per class\n",
                "for cls in classes:\n",
                "    cls_path = os.path.join(data_dir, cls)\n",
                "    if os.path.isdir(cls_path):\n",
                "        num_images = len([f for f in os.listdir(cls_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
                "        print(f\"  {cls}: {num_images} images\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Configure GPU Memory Growth"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configure GPU memory growth\n",
                "gpus = tf.config.list_physical_devices('GPU')\n",
                "if gpus:\n",
                "    try:\n",
                "        for gpu in gpus:\n",
                "            tf.config.experimental.set_memory_growth(gpu, True)\n",
                "        print(f\"‚úÖ Configured {len(gpus)} GPU(s) with memory growth\")\n",
                "    except RuntimeError as e:\n",
                "        print(e)\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è No GPU found, using CPU\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Define Constants and Hyperparameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Constants\n",
                "IMG_SIZE = (224, 224)  # EfficientNetV2M default input size\n",
                "BATCH_SIZE = 32\n",
                "NUM_CLASSES = 8\n",
                "DATA_DIR = \"kvasir-dataset-v2/kvasir-dataset-v2/\"\n",
                "EPOCHS = 20\n",
                "LEARNING_RATE = 1e-5  # Lower learning rate for fine-tuning\n",
                "\n",
                "print(f\"Configuration:\")\n",
                "print(f\"  Image Size: {IMG_SIZE}\")\n",
                "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
                "print(f\"  Num Classes: {NUM_CLASSES}\")\n",
                "print(f\"  Epochs: {EPOCHS}\")\n",
                "print(f\"  Learning Rate: {LEARNING_RATE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Load and Prepare Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load dataset with train/val/test split\n",
                "def load_dataset(data_dir, img_size, batch_size):\n",
                "    # 70% training, 30% for validation + test\n",
                "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
                "        data_dir,\n",
                "        validation_split=0.3,\n",
                "        subset=\"training\",\n",
                "        seed=123,\n",
                "        image_size=img_size,\n",
                "        batch_size=batch_size,\n",
                "    )\n",
                "\n",
                "    val_test_ds = tf.keras.utils.image_dataset_from_directory(\n",
                "        data_dir,\n",
                "        validation_split=0.3,\n",
                "        subset=\"validation\",\n",
                "        seed=123,\n",
                "        image_size=img_size,\n",
                "        batch_size=batch_size,\n",
                "    )\n",
                "\n",
                "    # Split val_test_ds into validation and test sets (50/50 split of the 30%)\n",
                "    val_batches = tf.data.experimental.cardinality(val_test_ds).numpy()\n",
                "    val_ds = val_test_ds.take(val_batches // 2)\n",
                "    test_ds = val_test_ds.skip(val_batches // 2)\n",
                "\n",
                "    return train_ds, val_ds, test_ds\n",
                "\n",
                "# Preprocessing function\n",
                "def preprocess_data(image, label):\n",
                "    image = tf.keras.applications.efficientnet_v2.preprocess_input(image)\n",
                "    return image, label\n",
                "\n",
                "# Load datasets\n",
                "train_ds, val_ds, test_ds = load_dataset(DATA_DIR, IMG_SIZE, BATCH_SIZE)\n",
                "\n",
                "# Apply preprocessing and optimization\n",
                "train_ds = train_ds.map(preprocess_data).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
                "val_ds = val_ds.map(preprocess_data).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
                "test_ds = test_ds.map(preprocess_data).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
                "\n",
                "print(\"‚úÖ Datasets loaded and preprocessed!\")\n",
                "print(f\"  Training batches: {tf.data.experimental.cardinality(train_ds).numpy()}\")\n",
                "print(f\"  Validation batches: {tf.data.experimental.cardinality(val_ds).numpy()}\")\n",
                "print(f\"  Test batches: {tf.data.experimental.cardinality(test_ds).numpy()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Build EfficientNetV2M Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_efficientnet_v2_model(num_classes, img_size):\n",
                "    \"\"\"Build EfficientNetV2M model with custom classification head\"\"\"\n",
                "    base_model = tf.keras.applications.EfficientNetV2M(\n",
                "        input_shape=(img_size[0], img_size[1], 3),\n",
                "        include_top=False,\n",
                "        weights=\"imagenet\"\n",
                "    )\n",
                "    base_model.trainable = True  # Unfreeze for fine-tuning\n",
                "\n",
                "    inputs = tf.keras.Input(shape=(img_size[0], img_size[1], 3))\n",
                "    x = tf.keras.applications.efficientnet_v2.preprocess_input(inputs)\n",
                "    x = base_model(x, training=False)\n",
                "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
                "    x = tf.keras.layers.Dropout(0.2)(x)\n",
                "    outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
                "    \n",
                "    model = tf.keras.Model(inputs, outputs)\n",
                "    return model\n",
                "\n",
                "# Build model\n",
                "model = build_efficientnet_v2_model(NUM_CLASSES, IMG_SIZE)\n",
                "\n",
                "# Compile model\n",
                "model.compile(\n",
                "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
                "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
                "    metrics=[\"accuracy\"]\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Model built and compiled!\")\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Define Callbacks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
                "\n",
                "# Define callbacks\n",
                "callbacks = [\n",
                "    ModelCheckpoint(\n",
                "        filepath='efficientnet_v2m_best_model.h5',\n",
                "        save_weights_only=False,\n",
                "        monitor='val_accuracy',\n",
                "        mode='max',\n",
                "        save_best_only=True,\n",
                "        verbose=1\n",
                "    ),\n",
                "    EarlyStopping(\n",
                "        monitor='val_loss',\n",
                "        patience=5,\n",
                "        restore_best_weights=True,\n",
                "        verbose=1\n",
                "    ),\n",
                "    ReduceLROnPlateau(\n",
                "        monitor='val_loss',\n",
                "        factor=0.5,\n",
                "        patience=3,\n",
                "        min_lr=1e-7,\n",
                "        verbose=1\n",
                "    ),\n",
                "    CSVLogger(\n",
                "        'training_log.csv',\n",
                "        separator=\",\",\n",
                "        append=False\n",
                "    )\n",
                "]\n",
                "\n",
                "print(\"‚úÖ Callbacks configured!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Train Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nüöÄ Starting training...\\n\")\n",
                "\n",
                "history = model.fit(\n",
                "    train_ds,\n",
                "    epochs=EPOCHS,\n",
                "    validation_data=val_ds,\n",
                "    callbacks=callbacks,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "print(\"\\n‚úÖ Training complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Evaluate on Test Set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
                "\n",
                "print(\"\\nEvaluating model on test set...\")\n",
                "\n",
                "# Get predictions\n",
                "test_labels = np.concatenate([y for x, y in test_ds], axis=0)\n",
                "test_predictions_probs = model.predict(test_ds)\n",
                "test_predictions = np.argmax(test_predictions_probs, axis=1)\n",
                "\n",
                "# Calculate metrics\n",
                "accuracy = accuracy_score(test_labels, test_predictions)\n",
                "precision = precision_score(test_labels, test_predictions, average='weighted')\n",
                "recall = recall_score(test_labels, test_predictions, average='weighted')\n",
                "f1 = f1_score(test_labels, test_predictions, average='weighted')\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"TEST SET RESULTS\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Test Accuracy:  {accuracy:.4f}\")\n",
                "print(f\"Test Precision: {precision:.4f}\")\n",
                "print(f\"Test Recall:    {recall:.4f}\")\n",
                "print(f\"Test F1 Score:  {f1:.4f}\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Classification report\n",
                "class_names = sorted(os.listdir(DATA_DIR))\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(test_labels, test_predictions, target_names=class_names))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Plot Training History"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
                "\n",
                "# Plot loss\n",
                "axes[0].plot(history.history['loss'], label='Training Loss', marker='o')\n",
                "axes[0].plot(history.history['val_loss'], label='Validation Loss', marker='s')\n",
                "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
                "axes[0].set_xlabel('Epoch')\n",
                "axes[0].set_ylabel('Loss')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Plot accuracy\n",
                "axes[1].plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
                "axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
                "axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
                "axes[1].set_xlabel('Epoch')\n",
                "axes[1].set_ylabel('Accuracy')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n‚úÖ Training history plot saved!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Plot Confusion Matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import seaborn as sns\n",
                "\n",
                "# Compute confusion matrix\n",
                "cm = confusion_matrix(test_labels, test_predictions)\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(12, 10))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
                "            xticklabels=class_names, yticklabels=class_names,\n",
                "            cbar_kws={'label': 'Count'})\n",
                "plt.title('Confusion Matrix - EfficientNetV2M', fontsize=16, fontweight='bold')\n",
                "plt.ylabel('True Label', fontsize=12)\n",
                "plt.xlabel('Predicted Label', fontsize=12)\n",
                "plt.xticks(rotation=45, ha='right')\n",
                "plt.yticks(rotation=0)\n",
                "plt.tight_layout()\n",
                "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n‚úÖ Confusion matrix saved!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 13. Save Final Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Save metrics summary\n",
                "metrics_summary = {\n",
                "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
                "    'Value': [accuracy, precision, recall, f1]\n",
                "}\n",
                "\n",
                "df_metrics = pd.DataFrame(metrics_summary)\n",
                "df_metrics.to_csv('test_metrics_summary.csv', index=False)\n",
                "\n",
                "print(\"\\n‚úÖ Results saved!\")\n",
                "print(\"\\nFiles created:\")\n",
                "print(\"  - efficientnet_v2m_best_model.h5 (Best model weights)\")\n",
                "print(\"  - training_log.csv (Training history)\")\n",
                "print(\"  - training_history.png (Training plots)\")\n",
                "print(\"  - confusion_matrix.png (Confusion matrix)\")\n",
                "print(\"  - test_metrics_summary.csv (Test metrics)\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"üéâ TRAINING COMPLETE!\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 14. Download Results (Optional)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Uncomment to download files to your local machine\n",
                "# from google.colab import files\n",
                "\n",
                "# files.download('efficientnet_v2m_best_model.h5')\n",
                "# files.download('training_log.csv')\n",
                "# files.download('training_history.png')\n",
                "# files.download('confusion_matrix.png')\n",
                "# files.download('test_metrics_summary.csv')"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}